# -*- coding: utf-8 -*-
"""CVday1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tHxWcXbXKVL3flYsB2T9sOA09OlKMmCa
"""

# 1) Setup
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# 2) Data: transforms + loaders
# MNIST images are 28x28 grayscale with pixel values 0..1 after ToTensor().
# Normalize with mean=0.1307, std=0.3081 (standard for MNIST) to help training.
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_ds = datasets.MNIST(root="./data", train=True, download=True, transform=transform)
test_ds  = datasets.MNIST(root="./data", train=False, download=True, transform=transform)

train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
test_loader  = DataLoader(test_ds,  batch_size=1000, shuffle=False)

# 3) Model: simple MLP
# 28x28 = 784 input features. We'll use 2 hidden layers (128 and 64 units).
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)  # 10 classes (digits 0..9)

    def forward(self, x):
        # x: (batch, 1, 28, 28) -> flatten to (batch, 784)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        # CrossEntropyLoss expects raw logits (no softmax here)
        x = self.fc3(x)
        return x

model = MLP().to(device)

# 4) Loss + Optimizer
criterion = nn.CrossEntropyLoss()     # combines LogSoftmax + NLLLoss
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# 5) Training loop
def train_one_epoch(model, loader, optimizer, criterion):
    model.train()
    running_loss = 0.0
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)

        # Forward
        logits = model(images)
        loss = criterion(logits, labels)

        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    return running_loss / len(loader)

# 6) Evaluation
@torch.no_grad()
def evaluate(model, loader, criterion):
    model.eval()
    total, correct, total_loss = 0, 0, 0.0
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        logits = model(images)
        loss = criterion(logits, labels)
        total_loss += loss.item()

        preds = logits.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    avg_loss = total_loss / len(loader)
    accuracy = 100.0 * correct / total
    return avg_loss, accuracy

# 7) Run training for ~5 epochs
EPOCHS = 5
for epoch in range(1, EPOCHS + 1):
    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)
    val_loss, val_acc = evaluate(model, test_loader, criterion)
    print(f"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | "
          f"Test Loss: {val_loss:.4f} | Test Acc: {val_acc:.2f}%")

# 8) Save the trained model (optional)
torch.save(model.state_dict(), "mnist_mlp.pth")
print("Saved model to mnist_mlp.pth")